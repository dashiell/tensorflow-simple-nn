{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow with simple 3-layer neural network using feedforward/backprop. \n",
    "\n",
    "This type of neural network gives no explanation of time or order.  It's not optimal for image classification and is just a demo, using the example MNIST data set provided by google. For better image classification performance, see convolutional neural networks.\n",
    "\n",
    "Credits to Magnus Erik Hvass Pedersen's excellent series of [TensorFlow tutorials](https://github.com/Hvass-Labs/TensorFlow-Tutorials)  and Harrison's wonderful series on [Practical Machine Learning with Python](https://pythonprogramming.net/machine-learning-tutorials/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries & Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "training size: 55000\n",
      "test size: 10000\n",
      "first 3 labels of test set: [7 2 1]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(train_dir=\"data/mnist\", one_hot = False)\n",
    "\n",
    "print(\"\\ntraining size: {}\".format(mnist.train.num_examples))\n",
    "print(\"test size: {}\".format(mnist.test.num_examples)) # Unused valid. set.\n",
    "\n",
    "print(\"first 3 labels of test set: {}\".format(mnist.test.labels[0:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize some variables and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_classes = 10 # numbers 0 - 9 in the dataset\n",
    "\n",
    "train_batch_size = 128 # manipulate weights by 128 features at a time\n",
    "\n",
    "image_size_1d = 28 * 28 # input images are 28 x 28\n",
    "\n",
    "# None indicates the first dimension (corresponding to batch size) can be any size\n",
    "x = tf.placeholder(dtype='float', shape=[None, image_size_1d], name='x') # height, width\n",
    "y_true = tf.placeholder(dtype='int32', name='y_true')\n",
    "\n",
    "total_iters = 15 # the number of times we will run our optimization over the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the graph.  Weights are not optimized here, but in training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X):\n",
    "    # number of hidden nodes each layer\n",
    "    n_nodes_hl1 = 700\n",
    "    n_nodes_hl2 = 700\n",
    "    n_nodes_hl3 = 700\n",
    "    \n",
    "    hidden_layer_1 = {'weights': tf.Variable(tf.random_normal([image_size_1d, n_nodes_hl1])), \n",
    "                     'biases' : tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "    \n",
    "    hidden_layer_2 = {'weights': tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])), \n",
    "                     'biases' : tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "    \n",
    "    hidden_layer_3 = {'weights': tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])), \n",
    "                     'biases' : tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "    \n",
    "    output_layer = {'weights': tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])), \n",
    "                     'biases' : tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    # X * weights + biases\n",
    "    l1 = tf.add(tf.matmul(X, hidden_layer_1['weights']), hidden_layer_1['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "    \n",
    "    l2 = tf.add(tf.matmul(l1, hidden_layer_2['weights']), hidden_layer_2['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "    \n",
    "    l3 = tf.add(tf.matmul(l2, hidden_layer_3['weights']), hidden_layer_3['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "    \n",
    "    predicted_class = tf.add(tf.matmul(l3, output_layer['weights']), output_layer['biases'])\n",
    "    \n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Optimize. We pass the training set to train_model and make a prediciton based on the output of the model. \n",
    "\n",
    "cross-entropy is a continuous function that is always positive. If the predicted y equals the true y, cross-entropy equals zero.  Cross-entropy is used in classification.  This measures how well the model works on each image individually. The softmax_cross_entropy_with_logits function makes the sum of the inputs equal to 1, normalizing the values\n",
    "\n",
    "cost is what we seek to miminimize.  It is a scalar representing the average cross-entropy for the classification of all images.  \n",
    "\n",
    "optimizer minimizes cost by manipulating the weights and biases in the model. The optimization algorithm used to accomplish this is gradient descent.  Note that in the initialization, it is only being added to the graph and not executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(x):\n",
    "    y_pred_class = model(x)\n",
    "    \n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_pred_class, labels=y_true)\n",
    "    cost = tf.reduce_mean( cross_entropy)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(cost)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # initialize the weights and biases before optimization starts\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # train the network\n",
    "        for iter in range(1, total_iters+1):\n",
    "            iter_loss = 0\n",
    "            for _ in range(int(mnist.train.num_examples / train_batch_size)):\n",
    "                # get the next batch of training examples. \n",
    "                # x_batch holds a batch of images\n",
    "                # y_true_batch holds the true labels for x_batch\n",
    "                x_batch, y_true_batch = mnist.train.next_batch(train_batch_size)\n",
    "                \n",
    "                # fd_train holds the batch in a dictionary with the named placeholders\n",
    "                # in the tensorflow graph\n",
    "                fd_train = {x: x_batch, y_true: y_true_batch}\n",
    "                _, i_loss = sess.run([optimizer, cost], feed_dict=fd_train)\n",
    "\n",
    "                iter_loss += i_loss   \n",
    "            print('iter', iter, 'of', total_iters, 'loss: ', iter_loss)\n",
    "    \n",
    "        correct = tf.nn.in_top_k(y_pred_class, y_true, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "        print('\\ntraining set accuracy: ', accuracy.eval({x: mnist.train.images, y_true: mnist.train.labels})) \n",
    "        print('test set accuracy: ', accuracy.eval({x: mnist.test.images, y_true: mnist.test.labels}))         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 of 15 loss:  2108923.88188\n",
      "iter 2 of 15 loss:  81097.015522\n",
      "iter 3 of 15 loss:  42694.2836113\n",
      "iter 4 of 15 loss:  24688.701741\n",
      "iter 5 of 15 loss:  16048.4512193\n",
      "iter 6 of 15 loss:  10551.8143533\n",
      "iter 7 of 15 loss:  7186.29605607\n",
      "iter 8 of 15 loss:  4823.72324636\n",
      "iter 9 of 15 loss:  3292.5080048\n",
      "iter 10 of 15 loss:  2412.67792343\n",
      "iter 11 of 15 loss:  1712.10381894\n",
      "iter 12 of 15 loss:  1149.28318126\n"
     ]
    }
   ],
   "source": [
    "train_model(x)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
